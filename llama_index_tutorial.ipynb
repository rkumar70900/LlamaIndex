{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiKey = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install OpenAI --quiet\n",
    "!pip3 install llama_index --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Basic Tutorial__\n",
    "<br>\n",
    "<br>\n",
    "Building an __LLM__ Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "response = OpenAI(api_key=apiKey).complete(\"Paul Graham is \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, OpenAI's gpt-4 model is used as an LLM along with OpenAI Embeddings model to create a vector store for the paul graham's essay which is in data folder.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "_ServiceContext_ - to personalize the application instead of defaults (like the above)\n",
    "<br>\n",
    "_SimpleDirectoryReader_ - to load data (documents) from the folder (in this case \"data\")\n",
    "<br>\n",
    "_VectorStoreIndex_ - to create a vector store from documents loaded using SimpleDirectoryReader\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "_llama_index.llms_ - has all the LLMs that can be used\n",
    "<br>\n",
    "_llama_index.embeddings_ - has all the embeddings that can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "llm = OpenAI(api_key=apiKey, temperature=0.1, model=\"gpt-4\")\n",
    "embed_model = OpenAIEmbedding(api_key=apiKey)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Data has to be loaded before any LLM can use it to answer our queries. This is a parallel to data cleaning/feature engineering/ETL pipelines.\n",
    "<br>\n",
    "<br>\n",
    "There are three main stages in this pipeline\n",
    "1. Load the data\n",
    "2. Transform the data\n",
    "3. Index and store the data\n",
    "\n",
    "<br>\n",
    "Various ways of ingesting data\n",
    "\n",
    "### Loaders\n",
    "Loading data is done using data connectors otherwise known as Reader. Data Connectors ingest data from different sources and format the data into Document objects. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "_Document_ -> It is a collection of data and metadata about that data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SimpleDirectoryReader\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of places we get data from and not everything is built in but there are a lot of connectors in LlamaHub\n",
    "<br>\n",
    "https://llamahub.ai/?tab=loaders\n",
    "<br>\n",
    "<br>\n",
    "In the below example, there is a database reader which can be used to connect to a data and load data for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "import os\n",
    "\n",
    "DatabaseReader = download_loader(\"DatabaseReader\")\n",
    "\n",
    "reader = DatabaseReader(\n",
    "    scheme=os.getenv(\"DB_SCHEME\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASS\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    ")\n",
    "\n",
    "query = \"SELECT * FROM users\"\n",
    "documents = reader.load_data(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In another world, we can write documents instead of loading them\n",
    "\n",
    "from llama_index.schema import Document\n",
    "\n",
    "doc = Document(text=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Data needs to be processed and transformed before placing it in a storage system. Transformations include chunking, extracting metadata, and embedding each chunk. \n",
    "<br>\n",
    "<br>\n",
    "Transformations input/outputs are Node objects (a Document is a subclass of a Node). These can be stacked and reordered.\n",
    "\n",
    "#### High Level Transformations API\n",
    "\n",
    "_.from_documents()_ method of VectorStoreIndex accepts an array of Document objects and will correctly parse and chunk them up.\n",
    "<br>\n",
    "<br>\n",
    "Under the hood, this splits the document into Node objects, which are similar to Documents (they contain text and metadata) but have relationship with parent Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Level Transformations API\n",
    "\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for more customization, like splitting the text into chuncks of said size SentenceSplitter can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, text_splitter=text_splitter)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower-Level Transformation API\n",
    "\n",
    "The above steps can be explicitly defined.\n",
    "\n",
    "It can be done either by using transformations modules (text splitters, metadata, extractors, etc.) as standalone components, or compose them in a declarative Transformation Pipeline Interface. \n",
    "<br>\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/root.html\n",
    "\n",
    "A key step is to split the documents into chunks/Node objects. The idea is to process the data into bite-size pieces that can be retrieved/fed to the LLM. These can used in their own or part of an ingestion pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.node_parser import TokenTextSplitter\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\n",
    "\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Metadata\n",
    "\n",
    "metadata can be added to documents or nodes either manually or with automatic metadata extractors.\n",
    "<br>\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(\n",
    "    text=\"text\",\n",
    "    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To insert a node into a vector index, it should have an embedding. nodes can created directly and passed on to an indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode\n",
    "\n",
    "node1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n",
    "node2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n",
    "\n",
    "index = VectorStoreIndex([node1, node2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents/Nodes\n",
    "Core abstractions within LlamaIndex.\n",
    "<br>\n",
    "<br>\n",
    "you can either use documents or nodes for creating a vector store\n",
    "<br>\n",
    "<br>\n",
    "A Document is the entire data source - for instance a PDF, an API output, or retrieved data from a database.\n",
    "<br>\n",
    "<br>\n",
    "A Node represents a chunck of the source document.\n",
    "<br>\n",
    "<br> \n",
    "Both the document and node can contains metadata and relationship to other documents/nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage pattern\n",
    "# Documents\n",
    "from llama_index import Document, VectorStoreIndex\n",
    "\n",
    "text_list = ['text1', 'text2']\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index import Document, VectorStoreIndex\n",
    "\n",
    "# load documents\n",
    "text_list = ['text1', 'text2']\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "\n",
    "# parse nodes (spliiting data into chunks)\n",
    "parser = SentenceSplitter()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
