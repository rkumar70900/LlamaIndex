{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiKey = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install OpenAI --quiet\n",
    "!pip3 install llama_index --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Basic Tutorial__\n",
    "<br>\n",
    "<br>\n",
    "Building an __LLM__ Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "response = OpenAI(api_key=apiKey).complete(\"Paul Graham is \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, OpenAI's gpt-4 model is used as an LLM along with OpenAI Embeddings model to create a vector store for the paul graham's essay which is in data folder.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "_ServiceContext_ - to personalize the application instead of defaults (like the above)\n",
    "<br>\n",
    "_SimpleDirectoryReader_ - to load data (documents) from the folder (in this case \"data\")\n",
    "<br>\n",
    "_VectorStoreIndex_ - to create a vector store from documents loaded using SimpleDirectoryReader\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "_llama_index.llms_ - has all the LLMs that can be used\n",
    "<br>\n",
    "_llama_index.embeddings_ - has all the embeddings that can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "llm = OpenAI(api_key=apiKey, temperature=0.1, model=\"gpt-4\")\n",
    "embed_model = OpenAIEmbedding(api_key=apiKey)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Data has to be loaded before any LLM can use it to answer our queries. This is a parallel to data cleaning/feature engineering/ETL pipelines.\n",
    "<br>\n",
    "<br>\n",
    "There are three main stages in this pipeline\n",
    "1. Load the data\n",
    "2. Transform the data\n",
    "3. Index and store the data\n",
    "\n",
    "<br>\n",
    "Various ways of ingesting data\n",
    "\n",
    "### Loaders\n",
    "Loading data is done using data connectors otherwise known as Reader. Data Connectors ingest data from different sources and format the data into Document objects. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "_Document_ -> It is a collection of data and metadata about that data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SimpleDirectoryReader\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of places we get data from and not everything is built in but there are a lot of connectors in LlamaHub\n",
    "<br>\n",
    "https://llamahub.ai/?tab=loaders\n",
    "<br>\n",
    "<br>\n",
    "In the below example, there is a database reader which can be used to connect to a data and load data for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "import os\n",
    "\n",
    "DatabaseReader = download_loader(\"DatabaseReader\")\n",
    "\n",
    "reader = DatabaseReader(\n",
    "    scheme=os.getenv(\"DB_SCHEME\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASS\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    ")\n",
    "\n",
    "query = \"SELECT * FROM users\"\n",
    "documents = reader.load_data(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In another world, we can write documents instead of loading them\n",
    "\n",
    "from llama_index.schema import Document\n",
    "\n",
    "doc = Document(text=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Data needs to be processed and transformed before placing it in a storage system. Transformations include chunking, extracting metadata, and embedding each chunk. \n",
    "<br>\n",
    "<br>\n",
    "Transformations input/outputs are Node objects (a Document is a subclass of a Node). These can be stacked and reordered.\n",
    "\n",
    "#### High Level Transformations API\n",
    "\n",
    "_.from_documents()_ method of VectorStoreIndex accepts an array of Document objects and will correctly parse and chunk them up.\n",
    "<br>\n",
    "<br>\n",
    "Under the hood, this splits the document into Node objects, which are similar to Documents (they contain text and metadata) but have relationship with parent Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Level Transformations API\n",
    "\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for more customization, like splitting the text into chuncks of said size SentenceSplitter can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, text_splitter=text_splitter)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower-Level Transformation API\n",
    "\n",
    "The above steps can be explicitly defined.\n",
    "\n",
    "It can be done either by using transformations modules (text splitters, metadata, extractors, etc.) as standalone components, or compose them in a declarative Transformation Pipeline Interface. \n",
    "<br>\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/root.html\n",
    "\n",
    "A key step is to split the documents into chunks/Node objects. The idea is to process the data into bite-size pieces that can be retrieved/fed to the LLM. These can used in their own or part of an ingestion pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.node_parser import TokenTextSplitter\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\n",
    "\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Metadata\n",
    "\n",
    "metadata can be added to documents or nodes either manually or with automatic metadata extractors.\n",
    "<br>\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(\n",
    "    text=\"text\",\n",
    "    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To insert a node into a vector index, it should have an embedding. nodes can created directly and passed on to an indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode\n",
    "\n",
    "node1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n",
    "node2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n",
    "\n",
    "index = VectorStoreIndex([node1, node2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents/Nodes\n",
    "Core abstractions within LlamaIndex.\n",
    "<br>\n",
    "<br>\n",
    "you can either use documents or nodes for creating a vector store\n",
    "<br>\n",
    "<br>\n",
    "A Document is the entire data source - for instance a PDF, an API output, or retrieved data from a database.\n",
    "<br>\n",
    "<br>\n",
    "A Node represents a chunck of the source document.\n",
    "<br>\n",
    "<br> \n",
    "Both the document and node can contains metadata and relationship to other documents/nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage pattern\n",
    "# Documents\n",
    "from llama_index import Document, VectorStoreIndex\n",
    "\n",
    "text_list = ['text1', 'text2']\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index import Document, VectorStoreIndex\n",
    "\n",
    "# load documents\n",
    "text_list = ['text1', 'text2']\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "\n",
    "# parse nodes (spliiting data into chunks)\n",
    "parser = SentenceSplitter()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Customizing Documents\n",
    "\n",
    "metadata can be included in the documents that are being read. Any information set in the metadata dictionary of the document will come up in the metadata of each node created from that document. This enables the index to use those in queries and respones. As a  default, metadata is injected into the text for both embedding and LLM model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various ways to set up the dictionary\n",
    "\n",
    "# 1. Document Constructor\n",
    "document = Document(\n",
    "    text=\"text\",\n",
    "    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n",
    ")\n",
    "\n",
    "# 2. After the document is created\n",
    "document.metadata = {\"filename\": \"<doc_file_name>\"}\n",
    "\n",
    "# 3. set the filename automatically using SimpleDirectoryReader and file_metadata hook\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "filename_fn = lambda filename: {\"file_name\": filename}\n",
    "\n",
    "# automatically sets the metadata of each document according to filename_fn\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\", file_metadata=filename_fn\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing LLM Metadata Text\n",
    "\n",
    "We might not want certain metadata keys to be read by an LLM but is generates better embeddings. we can exclude those so that LLM can't read those but can be used by the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding file_name key from being read by LLM\n",
    "document.excluded_llm_metadata_keys = [\"file_name\"]\n",
    "\n",
    "# Testing what the LLM can read from the metadata\n",
    "from llama_index.schema import MetadataMode\n",
    "print(document.get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for embedding model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding file_name key from being read by embedding model\n",
    "document.excluded_embed_metadata_keys = [\"file_name\"]\n",
    "\n",
    "# Testing what the embedding model can read from the metadata\n",
    "print(document.get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing metadata format\n",
    "\n",
    "format of the metadata being included into the actual text of each node and document when sent to LLM or embedding model is controlled by these three attributes\n",
    "\n",
    "1. Document.metadata_seperator -> separator between each key/value pair\n",
    "2. Document.metadata_template -> how key value pairs are formatted (by default it is strings)\n",
    "3. Document.text_template -> what metadata looks like when joined with the text content of documents/nodes\n",
    "\n",
    "As a whole,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLM sees this: \n",
      " Metadata: category=>finance::author=>LlamaIndex\n",
      "-----\n",
      "Content: This is a super-customized document\n",
      "The Embedding model sees this: \n",
      " Metadata: file_name=>super_secret_document.txt::category=>finance::author=>LlamaIndex\n",
      "-----\n",
      "Content: This is a super-customized document\n"
     ]
    }
   ],
   "source": [
    "from llama_index import Document\n",
    "from llama_index.schema import MetadataMode\n",
    "\n",
    "document = Document(\n",
    "    text=\"This is a super-customized document\",\n",
    "    metadata={\n",
    "        \"file_name\": \"super_secret_document.txt\",\n",
    "        \"category\": \"finance\",\n",
    "        \"author\": \"LlamaIndex\",\n",
    "    },\n",
    "    excluded_llm_metadata_keys=[\"file_name\"],\n",
    "    metadata_seperator=\"::\",\n",
    "    metadata_template=\"{key}=>{value}\",\n",
    "    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\\n\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    document.get_content(metadata_mode=MetadataMode.LLM),\n",
    ")\n",
    "print(\n",
    "    \"The Embedding model sees this: \\n\",\n",
    "    document.get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a list of nodes\n",
    "nodes = ...\n",
    "\n",
    "# Loop through each node\n",
    "for node in nodes:\n",
    "    # Access the metadata attribute\n",
    "    metadata = node.metadata\n",
    "\n",
    "    # The metadata is a dictionary, you can access the extracted titles or questions like this:\n",
    "    document_title = metadata.get('document_title')\n",
    "    questions_this_excerpt_can_answer = metadata.get('questions_this_excerpt_can_answer')\n",
    "\n",
    "    print(f\"Document Title: {document_title}\")\n",
    "    print(f\"Questions this excerpt can answer: {questions_this_excerpt_can_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Extraction\n",
    "\n",
    "metadata can be extracted from using LLMs with Metadata Extractor modules.\n",
    "\n",
    "1. SummaryExtractor - extracts a summary over a set of Nodes\n",
    "2. QuestionsAnsweredExtractor - extracts a set of questions that each Node can answer\n",
    "3. TitleExtractor - extracts a title over the context of each Node\n",
    "4. EntityExtractor -  extracts entities (i.e. names of places, people, things) mentioned in the content of each Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors import (\n",
    "    TitleExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=128\n",
    ")\n",
    "title_extractor = TitleExtractor(nodes=5)\n",
    "qa_extractor = QuestionsAnsweredExtractor(questions=3)\n",
    "\n",
    "# assume documents are defined -> extract nodes\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[text_splitter, title_extractor, qa_extractor]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    documents=documents,\n",
    "    in_place=True,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# or use it in the ServieContext\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    transformations=[text_splitter, title_extractor, qa_extractor]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
